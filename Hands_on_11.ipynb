{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N1-2CknMQKvi"
      },
      "source": [
        "### Objective\n",
        "\n",
        "In this notebook, learners will understand the concepts of **caching** and **broadcast variables** in PySpark.\n",
        "\n",
        "They will:\n",
        "\n",
        "- Create and explore large and small DataFrames\n",
        "- Use caching to avoid repeated computation\n",
        "- Use broadcast joins to optimize performance\n",
        "\n",
        "These optimizations are essential for working efficiently with large-scale distributed data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_2Plr9AWAl09"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, broadcast\n",
        "\n",
        "# Create a Spark session â€” the entry point to use PySpark\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"CachingAndBroadcasting\") \\\n",
        "    .getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eMldvBlvBEPf"
      },
      "outputs": [],
      "source": [
        "# Generate a large dataset: 1 million sales records\n",
        "# Each record contains: sale_id, user_id (user_0 to user_99), state_code (0 to 49)\n",
        "sales_data = [(i, f\"user_{i%100}\", i % 50) for i in range(1_000_000)]\n",
        "\n",
        "# Create a DataFrame from the sales data\n",
        "sales_df = spark.createDataFrame(sales_data, [\"sale_id\", \"user_id\", \"state_code\"])\n",
        "\n",
        "# Create a small lookup DataFrame for state codes\n",
        "states = [\n",
        "    (0, \"Andhra Pradesh\"),\n",
        "    (1, \"Bihar\"),\n",
        "    (2, \"Delhi\"),\n",
        "    (3, \"Karnataka\"),\n",
        "    (4, \"Maharashtra\")\n",
        "] + [(i, f\"State_{i}\") for i in range(5, 50)]  # Generic names for remaining codes\n",
        "\n",
        "states_df = spark.createDataFrame(states, [\"state_code\", \"state_name\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B6V-KwffBIWO",
        "outputId": "15b1ec92-f771-4294-ed5f-f1f66b944f3c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------+-----+\n",
            "|state_code|count|\n",
            "+----------+-----+\n",
            "|         0|20000|\n",
            "|         7|20000|\n",
            "|         6|20000|\n",
            "|         9|20000|\n",
            "|         5|20000|\n",
            "|         1|20000|\n",
            "|         3|20000|\n",
            "|         8|20000|\n",
            "|         2|20000|\n",
            "|         4|20000|\n",
            "+----------+-----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Caching Example\n",
        "\n",
        "# Filter the large dataset to only include state codes less than 10\n",
        "# This simulates a heavy computation\n",
        "filtered_sales = sales_df.filter(col(\"state_code\") < 10)\n",
        "\n",
        "# Cache the filtered DataFrame in memory\n",
        "# This means Spark will store the result after the first computation\n",
        "filtered_sales.cache()\n",
        "\n",
        "# First action: group by state_code and count sales\n",
        "# This triggers computation and caches the result\n",
        "filtered_sales.groupBy(\"state_code\").count().show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SAGdtatMOKcA",
        "outputId": "76d3dec8-c228-49c7-e3d9-5157796fb4d2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "20"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Second action: count distinct user IDs\n",
        "# Since the DataFrame is cached, Spark reuses the stored result\n",
        "filtered_sales.select(\"user_id\").distinct().count()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JtTV2r7SfqBL"
      },
      "source": [
        "### What Just Happened?\n",
        "\n",
        "- We filtered the DataFrame on `state_code < 10`, simulating a time-consuming operation.\n",
        "- By using `.cache()`, we stored this result in memory.\n",
        "- The **first action** (`groupBy + count`) triggered the computation and cached the data.\n",
        "- The **second action** reused the cached result without repeating the computation.\n",
        "\n",
        "**Why use caching?**\n",
        "- It reduces execution time when reusing the same filtered or transformed data multiple times."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a8CvwP3Te7FN",
        "outputId": "0be5a853-91eb-45c0-e51c-12ef7118feb8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------+----------+\n",
            "|sale_id|state_name|\n",
            "+-------+----------+\n",
            "|     26|  State_26|\n",
            "|     29|  State_29|\n",
            "|     76|  State_26|\n",
            "|     79|  State_29|\n",
            "|    126|  State_26|\n",
            "+-------+----------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Broadcast Example\n",
        "# Regular join: joins sales_df with states_df using a shuffle\n",
        "joined_normal = sales_df.join(states_df, on=\"state_code\")\n",
        "joined_normal.select(\"sale_id\", \"state_name\").show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vqqGREicfBoM",
        "outputId": "b7aecf7d-5093-4cee-9935-f7b515cd2a7d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------+--------------+\n",
            "|sale_id|    state_name|\n",
            "+-------+--------------+\n",
            "|      0|Andhra Pradesh|\n",
            "|      1|         Bihar|\n",
            "|      2|         Delhi|\n",
            "|      3|     Karnataka|\n",
            "|      4|   Maharashtra|\n",
            "+-------+--------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Optimized join: broadcast the small lookup table\n",
        "# Spark sends a copy of 'states_df' to all executor nodes\n",
        "joined_broadcast = sales_df.join(broadcast(states_df), on=\"state_code\")\n",
        "joined_broadcast.select(\"sale_id\", \"state_name\").show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RBB38OFif0VK"
      },
      "source": [
        "### What Just Happened?\n",
        "\n",
        "- In the **normal join**, Spark performs a full shuffle of `states_df` across partitions.\n",
        "- In the **broadcast join**, Spark sends the small DataFrame (`states_df`) to each executor.\n",
        "\n",
        "**Why broadcast?**\n",
        "- It avoids network shuffling.\n",
        "- Great for joining large DataFrames with smaller lookup tables.\n",
        "- Improves performance significantly in distributed environments.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JUnYPfgzY_u6"
      },
      "source": [
        "### Summary\n",
        "\n",
        "In this notebook, you learned:\n",
        "\n",
        "- How to use **caching** to store intermediate results in memory\n",
        "- How to use **broadcast variables** to optimize joins with small DataFrames\n",
        "\n",
        "These techniques are essential for building efficient data pipelines with PySpark.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
